{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fa19d2c1",
   "metadata": {
    "cellId": "5f1omla8309cra9gfjvefp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/resources/SR-Gaming-Bench/train\n"
     ]
    }
   ],
   "source": [
    "%cd ../train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ddfe9fcd",
   "metadata": {
    "cellId": "fexc0xzs5uvolxf7qq1ot"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import emt.archs  # noqa\n",
    "import emt.data  # noqa\n",
    "import emt.models  # noqa\n",
    "import torch\n",
    "from basicsr.data.prefetch_dataloader import CPUPrefetcher, CUDAPrefetcher\n",
    "from basicsr.models import build_model\n",
    "from basicsr.train import (\n",
    "    create_train_val_dataloader,\n",
    "    init_tb_loggers,\n",
    "    load_resume_state,\n",
    ")\n",
    "from basicsr.utils import (\n",
    "    AvgTimer,\n",
    "    MessageLogger,\n",
    "    get_env_info,\n",
    "    get_root_logger,\n",
    "    get_time_str,\n",
    "    make_exp_dirs,\n",
    "    mkdir_and_rename,\n",
    "    set_random_seed,\n",
    ")\n",
    "from basicsr.utils.dist_util import get_dist_info, init_dist\n",
    "from basicsr.utils.options import (\n",
    "    _postprocess_yml_value,\n",
    "    copy_opt_file,\n",
    "    dict2str,\n",
    "    yaml_load,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f3a88709",
   "metadata": {
    "cellId": "8muv7e5r8pp78l4vb9llc"
   },
   "outputs": [],
   "source": [
    "def parse_options(\n",
    "    root_path: str,\n",
    "    opt_path: str,\n",
    "    is_train: bool = True,\n",
    "    options_in_root: bool = False,\n",
    "    launcher: str = None,\n",
    "    auto_resume: bool = False,\n",
    "    debug: bool = False,\n",
    "    force_yml: list[str] = None,\n",
    ") -> dict[str, Any]:\n",
    "    # TODO написать docstring\n",
    "    if options_in_root:\n",
    "        opt = yaml_load(os.path.join(root_path, opt_path))\n",
    "    else:\n",
    "        opt = yaml_load(opt_path)\n",
    "\n",
    "    if launcher is None:\n",
    "        opt[\"dist\"] = False\n",
    "        print(\"Disable distributed.\", flush=True)\n",
    "    else:\n",
    "        opt[\"dist\"] = True\n",
    "        if launcher == \"slurm\" and \"dist_params\" in opt:\n",
    "            init_dist(launcher, **opt[\"dist_params\"])\n",
    "        else:\n",
    "            init_dist(launcher)\n",
    "    opt[\"rank\"], opt[\"world_size\"] = get_dist_info()\n",
    "\n",
    "    seed = opt.get(\"manual_seed\")\n",
    "    if seed is None:\n",
    "        seed = random.randint(1, 10000)\n",
    "        opt[\"manual_seed\"] = seed\n",
    "    set_random_seed(seed + opt[\"rank\"])\n",
    "\n",
    "    if force_yml is not None:\n",
    "        for entry in force_yml:\n",
    "            keys, value = entry.split(\"=\")\n",
    "            keys, value = keys.strip(), value.strip()\n",
    "            value = _postprocess_yml_value(value)\n",
    "            eval_str = \"opt\"\n",
    "            for key in keys.split(\":\"):\n",
    "                eval_str += f'[\"{key}\"]'\n",
    "            eval_str += \"=value\"\n",
    "            exec(eval_str)\n",
    "\n",
    "    opt[\"auto_resume\"] = auto_resume\n",
    "    opt[\"is_train\"] = is_train\n",
    "\n",
    "    if debug and not opt[\"name\"].startswith(\"debug\"):\n",
    "        opt[\"name\"] = \"debug_\" + opt[\"name\"]\n",
    "\n",
    "    if opt[\"num_gpu\"] == \"auto\":\n",
    "        opt[\"num_gpu\"] = torch.cuda.device_count()\n",
    "\n",
    "    datasets = opt[\"datasets\"][opt[\"datasets\"][\"type\"]]\n",
    "    for phase, dataset in datasets.items():\n",
    "        phase = phase.split(\"_\")[0]\n",
    "        dataset[\"phase\"] = phase\n",
    "        if \"scale\" in opt:\n",
    "            dataset[\"scale\"] = opt[\"scale\"]\n",
    "        if dataset.get(\"dataroot_gt\") is not None:\n",
    "            dataset[\"dataroot_gt\"] = os.path.expanduser(dataset[\"dataroot_gt\"])\n",
    "        if dataset.get(\"dataroot_lq\") is not None:\n",
    "            dataset[\"dataroot_lq\"] = os.path.expanduser(dataset[\"dataroot_lq\"])\n",
    "\n",
    "    for key, val in opt[\"path\"].items():\n",
    "        if (val is not None) and (\"resume_state\" in key or \"pretrain_network\" in key):\n",
    "            opt[\"path\"][key] = os.path.expanduser(val)\n",
    "\n",
    "    if is_train:\n",
    "        experiments_root = opt[\"path\"].get(\"experiments_root\")\n",
    "        if experiments_root is None:\n",
    "            experiments_root = os.path.join(root_path, \"experiments\")\n",
    "        experiments_root = os.path.join(experiments_root, opt[\"name\"])\n",
    "\n",
    "        opt[\"path\"][\"experiments_root\"] = experiments_root\n",
    "        opt[\"path\"][\"models\"] = os.path.join(experiments_root, \"models\")\n",
    "        opt[\"path\"][\"training_states\"] = os.path.join(\n",
    "            experiments_root, \"training_states\"\n",
    "        )\n",
    "        opt[\"path\"][\"log\"] = experiments_root\n",
    "        opt[\"path\"][\"visualization\"] = os.path.join(experiments_root, \"visualization\")\n",
    "\n",
    "        if \"debug\" in opt[\"name\"]:\n",
    "            if \"val\" in opt:\n",
    "                opt[\"val\"][\"val_freq\"] = 8\n",
    "            opt[\"logger\"][\"print_freq\"] = 1\n",
    "            opt[\"logger\"][\"save_checkpoint_freq\"] = 8\n",
    "    else:\n",
    "        results_root = opt[\"path\"].get(\"results_root\")\n",
    "        if results_root is None:\n",
    "            results_root = os.path.join(root_path, \"results\")\n",
    "        results_root = os.path.join(results_root, opt[\"name\"])\n",
    "\n",
    "        opt[\"path\"][\"results_root\"] = results_root\n",
    "        opt[\"path\"][\"log\"] = results_root\n",
    "        opt[\"path\"][\"visualization\"] = os.path.join(results_root, \"visualization\")\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "89ad7b78",
   "metadata": {
    "cellId": "tcxjk94ljlg4ocyq1wr8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disable distributed.\n"
     ]
    }
   ],
   "source": [
    "root = Path.cwd().parents[0]\n",
    "exp_folder = root / \"experiments\"\n",
    "os.makedirs(exp_folder, exist_ok=True)\n",
    "\n",
    "opt_path = str(root / \"configs/train/finetune_emt_x4_game_engine.yaml\")\n",
    "opt = parse_options(root, opt_path=opt_path, is_train=True, options_in_root=True)\n",
    "opt[\"root_path\"] = root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9df33cf9",
   "metadata": {
    "cellId": "2qghfy797ivtpqq1iilvic"
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "dc7740d3",
   "metadata": {
    "cellId": "vcr8quqpbihgg23ze5sf"
   },
   "outputs": [],
   "source": [
    "resume_state = load_resume_state(opt)\n",
    "\n",
    "if resume_state is None:\n",
    "    make_exp_dirs(opt)\n",
    "    if (\n",
    "        opt[\"logger\"].get(\"use_tb_logger\")\n",
    "        and \"debug\" not in opt[\"name\"]\n",
    "        and opt[\"rank\"] == 0\n",
    "    ):\n",
    "        mkdir_and_rename(os.path.join(exp_folder, \"tb_logger\", opt[\"name\"]))\n",
    "\n",
    "copy_opt_file(root / opt_path, opt[\"path\"][\"experiments_root\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "20902887",
   "metadata": {
    "cellId": "lw6z5twz6lx77wn2a6jl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 18:45:11,815 INFO: \n",
      "                ____                _       _____  ____\n",
      "               / __ ) ____ _ _____ (_)_____/ ___/ / __ \\\n",
      "              / __  |/ __ `// ___// // ___/\\__ \\ / /_/ /\n",
      "             / /_/ // /_/ /(__  )/ // /__ ___/ // _, _/\n",
      "            /_____/ \\__,_//____//_/ \\___//____//_/ |_|\n",
      "     ______                   __   __                 __      __\n",
      "    / ____/____   ____   ____/ /  / /   __  __ _____ / /__   / /\n",
      "   / / __ / __ \\ / __ \\ / __  /  / /   / / / // ___// //_/  / /\n",
      "  / /_/ // /_/ // /_/ // /_/ /  / /___/ /_/ // /__ / /<    /_/\n",
      "  \\____/ \\____/ \\____/ \\____/  /_____/\\____/ \\___//_/|_|  (_)\n",
      "    \n",
      "Version Information: \n",
      "\tBasicSR: 1.4.2\n",
      "\tPyTorch: 2.1.2+cu121\n",
      "\tTorchVision: 0.16.2+cu121\n",
      "2024-02-22 18:45:11,816 INFO: \n",
      "  name: EMT\n",
      "  model_type: IRModel\n",
      "  num_gpu: 1\n",
      "  manual_seed: 42\n",
      "  bit: 8\n",
      "  scale: 4\n",
      "  datasets:[\n",
      "    type: files\n",
      "    huggingface:[\n",
      "      huggingface_dir: epishchik/super-resolution-games\n",
      "      train:[\n",
      "        name: GameEngine_train\n",
      "        project: GameEngine_All\n",
      "        gt_size: 256\n",
      "        use_hflip: True\n",
      "        use_rot: True\n",
      "        use_shuffle: True\n",
      "        num_worker_per_gpu: 8\n",
      "        batch_size_per_gpu: 12\n",
      "        dataset_enlarge_ratio: 1\n",
      "        prefetch_mode: None\n",
      "      ]\n",
      "      val:[\n",
      "        name: GameEngine_val\n",
      "        project: GameEngine_All\n",
      "      ]\n",
      "    ]\n",
      "    files:[\n",
      "      train:[\n",
      "        name: GameEngine_train\n",
      "        type: IRDataset\n",
      "        dataroot_gt: datasets/GameEngineData/1080p/train\n",
      "        dataroot_lq: datasets/GameEngineData/270p/train\n",
      "        io_backend:[\n",
      "          type: disk\n",
      "        ]\n",
      "        gt_size: 256\n",
      "        use_hflip: True\n",
      "        use_rot: True\n",
      "        use_shuffle: True\n",
      "        num_worker_per_gpu: 8\n",
      "        batch_size_per_gpu: 12\n",
      "        dataset_enlarge_ratio: 1\n",
      "        prefetch_mode: None\n",
      "        bit: 8\n",
      "        phase: train\n",
      "        scale: 4\n",
      "      ]\n",
      "      val:[\n",
      "        name: GameEngine_val\n",
      "        type: IRDataset\n",
      "        dataroot_gt: datasets/GameEngineData/1080p/val\n",
      "        dataroot_lq: datasets/GameEngineData/270p/val\n",
      "        io_backend:[\n",
      "          type: disk\n",
      "        ]\n",
      "        bit: 8\n",
      "        phase: val\n",
      "        scale: 4\n",
      "      ]\n",
      "    ]\n",
      "  ]\n",
      "  network_g:[\n",
      "    type: EMT\n",
      "    dim: 60\n",
      "    n_blocks: 6\n",
      "    n_layers: 6\n",
      "    n_GTLs: 2\n",
      "    num_heads: 3\n",
      "    mlp_ratio: 2\n",
      "    window_list: [[32, 8], [8, 32]]\n",
      "    shift_list: [[16, 4], [4, 16]]\n",
      "    upscale: 4\n",
      "    num_in_ch: 3\n",
      "    num_out_ch: 3\n",
      "    task: lsr\n",
      "  ]\n",
      "  network_prefix:[\n",
      "    network_g: net_g\n",
      "  ]\n",
      "  path:[\n",
      "    pretrain_network_g: dvc_data/weights/emt/EMT_LSR_x4.pth\n",
      "    strict_load_g: True\n",
      "    resume_state: None\n",
      "    experiments_root: /home/jupyter/work/resources/SR-Gaming-Bench/experiments/EMT\n",
      "    models: /home/jupyter/work/resources/SR-Gaming-Bench/experiments/EMT/models\n",
      "    training_states: /home/jupyter/work/resources/SR-Gaming-Bench/experiments/EMT/training_states\n",
      "    log: /home/jupyter/work/resources/SR-Gaming-Bench/experiments/EMT\n",
      "    visualization: /home/jupyter/work/resources/SR-Gaming-Bench/experiments/EMT/visualization\n",
      "  ]\n",
      "  train:[\n",
      "    total_iter: 100000\n",
      "    optim_g:[\n",
      "      type: Adam\n",
      "      lr: 0.0005\n",
      "      weight_decay: 0\n",
      "      betas: [0.9, 0.999]\n",
      "    ]\n",
      "    scheduler:[\n",
      "      type: CosineAnnealingRestartLR\n",
      "      periods: [100000]\n",
      "      restart_weights: [1]\n",
      "      eta_min: 1e-06\n",
      "    ]\n",
      "    pixel_opt:[\n",
      "      type: L1Loss\n",
      "      loss_weight: 1.0\n",
      "      reduction: mean\n",
      "    ]\n",
      "  ]\n",
      "  val:[\n",
      "    val_freq: 5000.0\n",
      "    save_img: False\n",
      "    pbar: True\n",
      "    metrics:[\n",
      "      psnr:[\n",
      "        type: calculate_psnr\n",
      "        crop_border: 4\n",
      "        test_y_channel: False\n",
      "      ]\n",
      "    ]\n",
      "  ]\n",
      "  logger:[\n",
      "    print_freq: 100\n",
      "    save_checkpoint_freq: 5000.0\n",
      "    use_tb_logger: False\n",
      "    wandb:[\n",
      "      project: None\n",
      "      resume_id: None\n",
      "    ]\n",
      "    mlflow:[\n",
      "      tracking_uri: http://127.0.0.1:5000\n",
      "      experiment: SRGB train\n",
      "      run: Finetune EMT_x4 GameEngineData\n",
      "      log_system_metrics: False\n",
      "    ]\n",
      "  ]\n",
      "  dist_params:[\n",
      "    backend: nccl\n",
      "    port: 29500\n",
      "  ]\n",
      "  dist: False\n",
      "  rank: 0\n",
      "  world_size: 1\n",
      "  auto_resume: False\n",
      "  is_train: True\n",
      "  root_path: /home/jupyter/work/resources/SR-Gaming-Bench\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_file = os.path.join(opt[\"path\"][\"log\"], f\"train_{opt['name']}_{get_time_str()}.log\")\n",
    "\n",
    "logger = get_root_logger(\n",
    "    logger_name=\"basicsr\", log_level=logging.INFO, log_file=log_file\n",
    ")\n",
    "logger.info(get_env_info())\n",
    "logger.info(dict2str(opt))\n",
    "\n",
    "tb_logger = init_tb_loggers(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d84ef6ae",
   "metadata": {
    "cellId": "yflufh994q0kzmc15h3gh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 18:45:14,207 INFO: Dataset [IRDataset] - GameEngine_train is built.\n",
      "2024-02-22 18:45:14,208 INFO: Training statistics:\n",
      "\tNumber of train images: 14431\n",
      "\tDataset enlarge ratio: 1\n",
      "\tBatch size per gpu: 12\n",
      "\tWorld size (gpu number): 1\n",
      "\tRequire iter number per epoch: 1203\n",
      "\tTotal epochs: 84; iters: 100000.\n",
      "2024-02-22 18:45:14,446 INFO: Dataset [IRDataset] - GameEngine_val is built.\n",
      "2024-02-22 18:45:14,448 INFO: Number of val images/folders in GameEngine_val: 3600\n"
     ]
    }
   ],
   "source": [
    "datasets_type = opt[\"datasets\"][\"type\"]\n",
    "if datasets_type == \"files\":\n",
    "    result = create_train_val_dataloader(opt, logger, datasets_type, root=root)\n",
    "    train_loader, train_sampler, val_loaders, total_epochs, total_iters = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e35fa54e",
   "metadata": {
    "cellId": "wtz8heu5ax8cb7yk023uy8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 18:45:15,223 INFO: Network [EMT] is created.\n",
      "2024-02-22 18:45:15,529 INFO: Loading EMT model from /home/jupyter/work/resources/SR-Gaming-Bench/dvc_data/weights/emt/EMT_LSR_x4.pth, with param key: [params].\n",
      "2024-02-22 18:45:15,587 INFO: Loss [L1Loss] is created.\n",
      "2024-02-22 18:45:15,588 WARNING: Params sub_mean.weight will not be optimized.\n",
      "2024-02-22 18:45:15,589 WARNING: Params sub_mean.bias will not be optimized.\n",
      "2024-02-22 18:45:15,590 WARNING: Params add_mean.weight will not be optimized.\n",
      "2024-02-22 18:45:15,591 WARNING: Params add_mean.bias will not be optimized.\n",
      "2024-02-22 18:45:15,593 WARNING: Params body.0.sa_list.2.token_mixer.mask will not be optimized.\n",
      "2024-02-22 18:45:15,593 WARNING: Params body.0.sa_list.3.token_mixer.mask will not be optimized.\n",
      "2024-02-22 18:45:15,594 WARNING: Params body.0.sa_list.4.token_mixer.mask will not be optimized.\n",
      "2024-02-22 18:45:15,595 WARNING: Params body.0.sa_list.5.token_mixer.mask will not be optimized.\n",
      "2024-02-22 18:45:15,596 WARNING: Params body.0.mlp_list.0.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,597 WARNING: Params body.0.mlp_list.0.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,598 WARNING: Params body.0.mlp_list.1.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,600 WARNING: Params body.0.mlp_list.1.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,601 WARNING: Params body.0.mlp_list.2.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,602 WARNING: Params body.0.mlp_list.2.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,603 WARNING: Params body.0.mlp_list.3.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,604 WARNING: Params body.0.mlp_list.3.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,605 WARNING: Params body.0.mlp_list.4.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,606 WARNING: Params body.0.mlp_list.4.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,607 WARNING: Params body.0.mlp_list.5.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,608 WARNING: Params body.0.mlp_list.5.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,609 WARNING: Params body.1.sa_list.2.token_mixer.mask will not be optimized.\n",
      "2024-02-22 18:45:15,611 WARNING: Params body.1.sa_list.3.token_mixer.mask will not be optimized.\n",
      "2024-02-22 18:45:15,612 WARNING: Params body.1.sa_list.4.token_mixer.mask will not be optimized.\n",
      "2024-02-22 18:45:15,613 WARNING: Params body.1.sa_list.5.token_mixer.mask will not be optimized.\n",
      "2024-02-22 18:45:15,614 WARNING: Params body.1.mlp_list.0.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,615 WARNING: Params body.1.mlp_list.0.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,616 WARNING: Params body.1.mlp_list.1.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,617 WARNING: Params body.1.mlp_list.1.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,618 WARNING: Params body.1.mlp_list.2.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,619 WARNING: Params body.1.mlp_list.2.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,620 WARNING: Params body.1.mlp_list.3.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,621 WARNING: Params body.1.mlp_list.3.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,624 WARNING: Params body.1.mlp_list.4.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,625 WARNING: Params body.1.mlp_list.4.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,627 WARNING: Params body.1.mlp_list.5.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,628 WARNING: Params body.1.mlp_list.5.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,629 WARNING: Params body.2.sa_list.2.token_mixer.mask will not be optimized.\n",
      "2024-02-22 18:45:15,630 WARNING: Params body.2.sa_list.3.token_mixer.mask will not be optimized.\n",
      "2024-02-22 18:45:15,631 WARNING: Params body.2.sa_list.4.token_mixer.mask will not be optimized.\n",
      "2024-02-22 18:45:15,632 WARNING: Params body.2.sa_list.5.token_mixer.mask will not be optimized.\n",
      "2024-02-22 18:45:15,633 WARNING: Params body.2.mlp_list.0.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,634 WARNING: Params body.2.mlp_list.0.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,635 WARNING: Params body.2.mlp_list.1.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,636 WARNING: Params body.2.mlp_list.1.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,639 WARNING: Params body.2.mlp_list.2.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,640 WARNING: Params body.2.mlp_list.2.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,641 WARNING: Params body.2.mlp_list.3.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,642 WARNING: Params body.2.mlp_list.3.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,643 WARNING: Params body.2.mlp_list.4.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,645 WARNING: Params body.2.mlp_list.4.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,646 WARNING: Params body.2.mlp_list.5.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,647 WARNING: Params body.2.mlp_list.5.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,648 WARNING: Params body.3.sa_list.2.token_mixer.mask will not be optimized.\n",
      "2024-02-22 18:45:15,650 WARNING: Params body.3.sa_list.3.token_mixer.mask will not be optimized.\n",
      "2024-02-22 18:45:15,651 WARNING: Params body.3.sa_list.4.token_mixer.mask will not be optimized.\n",
      "2024-02-22 18:45:15,652 WARNING: Params body.3.sa_list.5.token_mixer.mask will not be optimized.\n",
      "2024-02-22 18:45:15,653 WARNING: Params body.3.mlp_list.0.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,654 WARNING: Params body.3.mlp_list.0.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,654 WARNING: Params body.3.mlp_list.1.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,655 WARNING: Params body.3.mlp_list.1.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,656 WARNING: Params body.3.mlp_list.2.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,657 WARNING: Params body.3.mlp_list.2.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,658 WARNING: Params body.3.mlp_list.3.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,661 WARNING: Params body.3.mlp_list.3.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,662 WARNING: Params body.3.mlp_list.4.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,663 WARNING: Params body.3.mlp_list.4.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,664 WARNING: Params body.3.mlp_list.5.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,665 WARNING: Params body.3.mlp_list.5.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,666 WARNING: Params body.4.sa_list.2.token_mixer.mask will not be optimized.\n",
      "2024-02-22 18:45:15,669 WARNING: Params body.4.sa_list.3.token_mixer.mask will not be optimized.\n",
      "2024-02-22 18:45:15,670 WARNING: Params body.4.sa_list.4.token_mixer.mask will not be optimized.\n",
      "2024-02-22 18:45:15,670 WARNING: Params body.4.sa_list.5.token_mixer.mask will not be optimized.\n",
      "2024-02-22 18:45:15,671 WARNING: Params body.4.mlp_list.0.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,672 WARNING: Params body.4.mlp_list.0.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,674 WARNING: Params body.4.mlp_list.1.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,676 WARNING: Params body.4.mlp_list.1.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,676 WARNING: Params body.4.mlp_list.2.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,677 WARNING: Params body.4.mlp_list.2.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,679 WARNING: Params body.4.mlp_list.3.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,680 WARNING: Params body.4.mlp_list.3.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,681 WARNING: Params body.4.mlp_list.4.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,682 WARNING: Params body.4.mlp_list.4.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,683 WARNING: Params body.4.mlp_list.5.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,685 WARNING: Params body.4.mlp_list.5.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,686 WARNING: Params body.5.sa_list.2.token_mixer.mask will not be optimized.\n",
      "2024-02-22 18:45:15,687 WARNING: Params body.5.sa_list.3.token_mixer.mask will not be optimized.\n",
      "2024-02-22 18:45:15,688 WARNING: Params body.5.sa_list.4.token_mixer.mask will not be optimized.\n",
      "2024-02-22 18:45:15,689 WARNING: Params body.5.sa_list.5.token_mixer.mask will not be optimized.\n",
      "2024-02-22 18:45:15,690 WARNING: Params body.5.mlp_list.0.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,691 WARNING: Params body.5.mlp_list.0.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,693 WARNING: Params body.5.mlp_list.1.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,693 WARNING: Params body.5.mlp_list.1.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,694 WARNING: Params body.5.mlp_list.2.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,695 WARNING: Params body.5.mlp_list.2.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,696 WARNING: Params body.5.mlp_list.3.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,697 WARNING: Params body.5.mlp_list.3.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,697 WARNING: Params body.5.mlp_list.4.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,698 WARNING: Params body.5.mlp_list.4.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,699 WARNING: Params body.5.mlp_list.5.fc1.mask will not be optimized.\n",
      "2024-02-22 18:45:15,702 WARNING: Params body.5.mlp_list.5.fc2.mask will not be optimized.\n",
      "2024-02-22 18:45:15,707 INFO: Model [IRModel] is created.\n"
     ]
    }
   ],
   "source": [
    "model = build_model(opt, root=root)\n",
    "if resume_state:\n",
    "    model.resume_training(resume_state)\n",
    "    logger.info(\n",
    "        f\"Resuming training from epoch: {resume_state['epoch']},\"\n",
    "        f\"iter: {resume_state['iter']}.\"\n",
    "    )\n",
    "    start_epoch = resume_state[\"epoch\"]\n",
    "    current_iter = resume_state[\"iter\"]\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    current_iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "fb32f5c9",
   "metadata": {
    "cellId": "bv5dugy1lotwv6tyxp2yv"
   },
   "outputs": [],
   "source": [
    "msg_logger = MessageLogger(opt, current_iter, tb_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "055c6118",
   "metadata": {
    "cellId": "knzoddf9f0m542b9l98nhg"
   },
   "outputs": [],
   "source": [
    "prefetch_mode = opt[\"datasets\"][datasets_type][\"train\"].get(\"prefetch_mode\")\n",
    "if prefetch_mode is None or prefetch_mode == \"cpu\":\n",
    "    prefetcher = CPUPrefetcher(train_loader)\n",
    "elif prefetch_mode == \"cuda\":\n",
    "    prefetcher = CUDAPrefetcher(train_loader, opt)\n",
    "    logger.info(f\"Use {prefetch_mode} prefetch dataloader\")\n",
    "    if opt[\"datasets\"][datasets_type][\"train\"].get(\"pin_memory\") is not True:\n",
    "        raise ValueError(\"Please set pin_memory=True for CUDAPrefetcher.\")\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Wrong prefetch_mode {prefetch_mode}. \"\n",
    "        f\"Supported ones are: None, 'cuda', 'cpu'.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bebc32d",
   "metadata": {
    "cellId": "fz04qxd2rioiiakrxjso9p"
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Start training from epoch: {start_epoch}, iter: {current_iter}\")\n",
    "data_timer, iter_timer = AvgTimer(), AvgTimer()\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, total_epochs + 1):\n",
    "    train_sampler.set_epoch(epoch)\n",
    "    prefetcher.reset()\n",
    "    train_data = prefetcher.next()\n",
    "\n",
    "    while train_data is not None:\n",
    "        data_timer.record()\n",
    "\n",
    "        current_iter += 1\n",
    "        if current_iter > total_iters:\n",
    "            break\n",
    "\n",
    "        model.update_learning_rate(\n",
    "            current_iter, warmup_iter=opt[\"train\"].get(\"warmup_iter\", -1)\n",
    "        )\n",
    "\n",
    "        model.feed_data(train_data)\n",
    "        model.optimize_parameters(current_iter)\n",
    "        iter_timer.record()\n",
    "        if current_iter == 1:\n",
    "            msg_logger.reset_start_time()\n",
    "        if current_iter % opt[\"logger\"][\"print_freq\"] == 0:\n",
    "            log_vars = {\"epoch\": epoch, \"iter\": current_iter}\n",
    "            log_vars.update({\"lrs\": model.get_current_learning_rate()})\n",
    "            log_vars.update(\n",
    "                {\n",
    "                    \"time\": iter_timer.get_avg_time(),\n",
    "                    \"data_time\": data_timer.get_avg_time(),\n",
    "                }\n",
    "            )\n",
    "            log_vars.update(model.get_current_log())\n",
    "            msg_logger(log_vars)\n",
    "\n",
    "        if current_iter % opt[\"logger\"][\"save_checkpoint_freq\"] == 0:\n",
    "            logger.info(\"Saving models and training states.\")\n",
    "            model.save(epoch, current_iter)\n",
    "\n",
    "        if opt.get(\"val\") is not None and (current_iter % opt[\"val\"][\"val_freq\"] == 0):\n",
    "            if len(val_loaders) > 1:\n",
    "                logger.warning(\n",
    "                    \"Multiple validation datasets are *only* supported by SRModel.\"\n",
    "                )\n",
    "            for val_loader in val_loaders:\n",
    "                model.validation(\n",
    "                    val_loader,\n",
    "                    current_iter,\n",
    "                    tb_logger,\n",
    "                    opt[\"val\"][\"save_img\"],\n",
    "                )\n",
    "\n",
    "        data_timer.start()\n",
    "        iter_timer.start()\n",
    "        train_data = prefetcher.next()\n",
    "\n",
    "consumed_time = str(datetime.timedelta(seconds=int(time.time() - start_time)))\n",
    "logger.info(f\"End of training. Time consumed: {consumed_time}\")\n",
    "logger.info(\"Save the latest model.\")\n",
    "\n",
    "model.save(epoch=-1, current_iter=-1)  # -1 stands for the latest\n",
    "if opt.get(\"val\") is not None:\n",
    "    for val_loader in val_loaders:\n",
    "        model.validation(val_loader, current_iter, tb_logger, opt[\"val\"][\"save_img\"])\n",
    "\n",
    "if tb_logger:\n",
    "    tb_logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e5ffca",
   "metadata": {
    "cellId": "bqfiphgdibjw0vwtz7qb5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "c0eca88b-be89-4704-8023-7d577b8e7dad",
  "notebookPath": "SR-Gaming-Bench/datasphere/train_emt.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
